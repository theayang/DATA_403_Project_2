{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-21T18:17:48.135418Z",
     "start_time": "2022-10-21T18:17:48.133240Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#pd.read_csv('application_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Implementation (Each function computes the current gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-25T01:43:52.334385Z",
     "start_time": "2022-10-25T01:43:51.961914Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_prob(X, B):\n",
    "    return 1 / (1 + np.exp(-1 * X @ B))\n",
    "\n",
    "# Logistic Regression Gradient\n",
    "def logistic(X, Y, B):\n",
    "    half =  np.exp(X @ B)\n",
    "    p = half / (1 + half)\n",
    "    gradient = -1 * (Y - p).T @ X\n",
    "    \n",
    "    return gradient\n",
    "\n",
    "# Logistic Regression with Lasso Penalty\n",
    "def logistic_lasso(X, Y, B, lamb):\n",
    "    half =  np.exp(X @ B)\n",
    "    p = half / (1 + half)\n",
    "    partial_gradient = -1 * (Y - p).T @ X\n",
    "    \n",
    "    # Add on +/- lambda * B to the gradient\n",
    "    lamb_beta = np.nan_to_num(lamb * -1 * (B / (B * -1)))\n",
    "    gradient = partial_gradient + lamb_beta\n",
    "    \n",
    "    return gradient\n",
    "\n",
    "# Implements SVC via some matrix magic. This requires 0/1 class labels and converts to 1/-1\n",
    "def SVC(X, Y, B, lamb):\n",
    "    # REQUIRES CLASS LABELS 1 / -1\n",
    "    Y = (Y - 1) + Y\n",
    "    \n",
    "    # I know this looks crazy but trust it works :)\n",
    "    mask = (1 - (Y * (X @ B))) <= 0\n",
    "\n",
    "    if_not_0_replace_w = (lamb * 2 * B.sum()) - (X.T * Y).T\n",
    "    return (2 * lamb * B.sum() * mask.sum() + if_not_0_replace_w[~mask].sum(axis=0)) / len(Y)\n",
    "\n",
    "# This uses lambda to minimize the loss due to bad observations and not Big betas. This should be equivalent\n",
    "# regular lambda ~ (1/2) * C\n",
    "def SVC_reverse_lambda(X, Y, B, C):\n",
    "    # REQUIRES CLASS LABELS 1 / -1\n",
    "    Y = (Y - 1) + Y\n",
    "    \n",
    "    # I know this looks crazy but trust it works :)\n",
    "    mask = (1 - (Y * (X @ B))) <= 0\n",
    "\n",
    "    if_not_0_replace_w = B - (C * (X.T * Y).T)\n",
    "    return (B * mask.sum() + if_not_0_replace_w[~mask].sum(axis=0)) / len(Y)\n",
    "\n",
    "from scipy import linalg\n",
    "def LDA(X, Y, w_B0=True):\n",
    "    if w_B0:\n",
    "        X = X.T[1:].T\n",
    "    X_group1 = X[Y == 1]\n",
    "    X_group2 = X[Y == 0]\n",
    "    \n",
    "    S1 = np.cov(X_group1.T)\n",
    "    S2 = np.cov(X_group2.T)\n",
    "\n",
    "    n1 = len(X_group1)\n",
    "    n2 = len(X_group2)\n",
    "    x_bar_1_2 = X_group1.mean(axis=0) - X_group2.mean(axis=0)\n",
    "    \n",
    "    Sp = ((n1 - 1) * S1 + (n2 - 1) * S2) / (n1 + n2 - 2)\n",
    "    \n",
    "    # if p == 1\n",
    "    if len(Sp.shape) != 0:\n",
    "        a = linalg.inv(Sp) @ x_bar_1_2\n",
    "    else:\n",
    "        a = (1 / Sp) * x_bar_1_2\n",
    "\n",
    "    \n",
    "    z_bar1 = (a @ X_group1.T).mean()\n",
    "    z_bar2 = (a @ X_group2.T).mean()\n",
    "\n",
    "    cutoff = .5 * (z_bar1 + z_bar2) - np.log(n1 / n2)\n",
    "    \n",
    "    # a * X >= cutoff -> class 1; else class 2\n",
    "    return a, cutoff\n",
    "\n",
    "def LDA_predict(a, cutoff, X, w_B0=True):\n",
    "    if w_B0:\n",
    "        X = X.T[1:].T\n",
    "    mask = a @ X.T >= cutoff\n",
    "    return np.where(mask, np.ones(len(X)), np.zeros(len(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-24T22:12:30.344036Z",
     "start_time": "2022-10-24T22:12:30.338481Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.796416881801354\n",
      "Accuracy: 0.78; Precision: 0.803; Recall: 0.781; f1: 0.792\n"
     ]
    }
   ],
   "source": [
    "a, cutoff = LDA(heart_X, heart_Y)\n",
    "predictions = LDA_predict(a, cutoff, heart_X)\n",
    "(predictions == heart_Y).sum() / len(heart_Y)\n",
    "compute_metrics(predictions, heart_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test code for logistic, SVC, and LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-21T18:16:40.032110Z",
     "start_time": "2022-10-21T18:16:38.774298Z"
    }
   },
   "outputs": [],
   "source": [
    "heart = pd.read_csv('https://www.dropbox.com/s/jpnyx41u7wpa41m/heart_attack_clean.csv?dl=1')\n",
    "heart_X = heart.drop(columns=['output'])\n",
    "heart_X['B0'] = 1\n",
    "heart_X = heart_X[['B0'] + list(heart_X.drop(columns='B0').columns)]\n",
    "heart_X = heart_X.to_numpy()\n",
    "heart_Y = heart['output'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test SVC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-25T00:25:18.635554Z",
     "start_time": "2022-10-25T00:25:18.507323Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eta: 0.1; Iterations: 278\n",
      "Gradient converged w/ 1510 iterations and eta = 0.01\n",
      "Accuracy: 0.791; Precision: 0.803; Recall: 0.808; f1: 0.805\n"
     ]
    }
   ],
   "source": [
    "heart_SVC_B = gradient_descent(SVC, heart_X, heart_Y, 5)\n",
    "predictions = np.where(heart_X @ heart_SVC_B >= 0, np.ones(len(z)), np.zeros(len(z)))\n",
    "compute_metrics(predictions, heart_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-22T18:14:12.663875Z",
     "start_time": "2022-10-22T18:14:12.654438Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8021978021978022"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "clf = sklearn.svm.SVC(kernel='linear')\n",
    "clf.fit(heart_X, heart_Y)\n",
    "clf.score(heart_X, heart_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test Logistic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-22T19:36:49.195467Z",
     "start_time": "2022-10-22T19:36:38.982862Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eta: 0.1; Iterations: 75000\n",
      "Eta: 0.01; Iterations: 75000\n",
      "Eta: 0.001; Iterations: 75000\n",
      "Gradient converged w/ 1999 iterations and eta = 0.0001\n",
      "Accuracy: 0.802; Precision: 0.799; Recall: 0.842; f1: 0.82\n"
     ]
    }
   ],
   "source": [
    "heart_logistic_lasso_B = gradient_descent(logistic_lasso, heart_X, heart_Y, 1, tol=1e-3)\n",
    "p_logistic = predict_prob(heart_X, heart_logistic_lasso_B)\n",
    "compute_metrics(p_logistic, heart_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-22T19:37:22.449529Z",
     "start_time": "2022-10-22T19:37:22.439905Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7985347985347986"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = sklearn.linear_model.LogisticRegression(fit_intercept=False)\n",
    "clf.fit(heart_X, heart_Y)\n",
    "clf.score(heart_X, heart_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LDA tests**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-22T20:03:38.089350Z",
     "start_time": "2022-10-22T20:03:38.080583Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.762; Precision: 0.787; Recall: 0.76; f1: 0.774\n"
     ]
    }
   ],
   "source": [
    "S1 = np.cov(heart_X.T[1:].T[heart_Y == 1].T)\n",
    "S2 = np.cov(heart_X.T[1:].T[heart_Y != 1].T)\n",
    "n1 = (heart_Y == 1).sum()\n",
    "n2 = (heart_Y != 1).sum()\n",
    "x_bar_1_2 = (heart_X[heart_Y == 1].mean(axis=0) - heart_X[heart_Y == 0].mean(axis=0))[1:]\n",
    "\n",
    "Sp = ((n1 - 1) * S1 + (n2 - 1) * S2) / (n1 + n2 - 2)\n",
    "a = np.linalg.inv(Sp) @ x_bar_1_2\n",
    "\n",
    "z = a.T @ heart_X.T[1:]\n",
    "z_bar1 = (a.T @ heart_X[heart_Y == 1].T[1:]).mean()\n",
    "z_bar2 = (a.T @ heart_X[heart_Y != 1].T[1:]).mean()\n",
    "\n",
    "cutoff = .5 * (z_bar1 + z_bar2)\n",
    "predictions = np.where(z >= cutoff, np.ones(len(z)), np.zeros(len(z)))\n",
    "(predictions == heart_Y).sum() / len(heart_Y)\n",
    "compute_metrics(predictions, heart_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-22T19:27:33.353148Z",
     "start_time": "2022-10-22T19:27:33.346679Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7948717948717948"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "clf = sklearn.discriminant_analysis.LinearDiscriminantAnalysis()\n",
    "clf.fit(heart_X, heart_Y)\n",
    "clf.score(heart_X, heart_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-21T18:16:40.708836Z",
     "start_time": "2022-10-21T18:16:40.701633Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_accuracy(p, Y):\n",
    "    return sum((p > .5) == Y) / len(Y)\n",
    "\n",
    "def get_precision(p, Y):\n",
    "    # precision = TP / (TP + FP)\n",
    "    TP = np.where(Y, ((p > .5) == Y), False).sum()\n",
    "    FP = np.where(Y == 0, (p > .5), False).sum()\n",
    "    return TP / (TP + FP)\n",
    "   \n",
    "def get_recall(p, Y):\n",
    "    # recall = TP / (TP + FN)\n",
    "    TP = np.where(Y, ((p > .5) == Y), False).sum()\n",
    "    FN = np.where(Y, ((p > .5) != Y), False).sum()\n",
    "    return TP / (TP + FN)\n",
    "\n",
    "def get_f1(p, Y):\n",
    "    precision = get_precision(p, Y)\n",
    "    recall = get_recall(p, Y)\n",
    "    return 2 / ((1 / precision) + (1 / recall))\n",
    "    \n",
    "# Compute Accuracy, precision, recall, and f1\n",
    "def compute_metrics(p, Y):\n",
    "    # precision = TP / (TP + FP)\n",
    "    # recall = TP / (TP + FN)\n",
    "    accuracy = sum((p > .5) == Y) / len(Y)\n",
    "    \n",
    "    TP = np.where(Y, ((p > .5) == Y), False).sum()\n",
    "    FP = np.where(Y == 0, (p > .5), False).sum()\n",
    "    precision = TP / (TP + FP)\n",
    "    \n",
    "    FN = np.where(Y, ((p > .5) != Y), False).sum()\n",
    "    recall = TP / (TP + FN)\n",
    "    \n",
    "    f1 = 2 / ((1 / precision) + (1 / recall))\n",
    "    print(f\"Accuracy: {round(accuracy, 3)}; Precision: {round(precision, 3)}; \" + \\\n",
    "           f\"Recall: {round(recall, 3)}; f1: {round(f1, 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Implementation (Pass in a regression function, X, Y, and any optional arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-21T18:17:32.220126Z",
     "start_time": "2022-10-21T18:17:32.210148Z"
    }
   },
   "outputs": [],
   "source": [
    "def gradient_descent(reg_func, X, Y, *reg_func_args, initial_B=None, max_iterations=75000, tol = .00001,\n",
    "                     etas=[.1, .01, .001, .0001, .00001, .000001], err=False):\n",
    "    if not err:\n",
    "        np.seterr(all=\"ignore\")\n",
    "    else:\n",
    "        np.seterr(all=\"warn\")\n",
    "    \n",
    "    for eta in etas:\n",
    "        # reset\n",
    "        iterations = 0\n",
    "        if initial_B is not None:\n",
    "            B = initial_B\n",
    "        else:\n",
    "            B = np.zeros(len(X[0]))\n",
    "        gradient = np.zeros(len(X[0]))\n",
    "        while iterations < max_iterations and np.isinf(B).sum() == 0 and \\\n",
    "              (iterations == 0 or (eta * (gradient ** 2)).sum() > tol):\n",
    "            # calls the regression function\n",
    "            gradient = reg_func(X, Y, B, *reg_func_args)\n",
    "            B = B - (eta * gradient)\n",
    "            iterations += 1\n",
    "\n",
    "        if iterations < max_iterations and np.isinf(B).sum() == 0 and np.isnan(B).sum() == 0:\n",
    "            print(f'Gradient converged w/ {iterations} iterations and eta = {eta}')\n",
    "            np.seterr(all=\"warn\")\n",
    "            return B\n",
    "        print(f'Eta: {eta}; Iterations: {iterations}')\n",
    "    print('GRADIENT DID NOT CONVERGE. RESULTS ARE BAD')\n",
    "    np.seterr(all=\"warn\")\n",
    "    return B\n",
    "\n",
    "# The code below uses the 'Adagrad' gradient descent optimization algorithm to adapt the \n",
    "# learning rate for each dimension. There are other versions of this that may be more effective\n",
    "# Directions as to how this works as well as other ideas: \n",
    "# https://ruder.io/optimizing-gradient-descent/index.html#momentum\n",
    "def adaptive_gradient_descent(reg_func, X, Y, *reg_func_args, initial_B=None, max_iterations=100000, \n",
    "                              tol = .001, etas=[.1], err=False):\n",
    "    if not err:\n",
    "        np.seterr(all=\"ignore\")\n",
    "    else:\n",
    "        np.seterr(all=\"warn\")\n",
    "    \n",
    "    for eta in etas:\n",
    "        # reset\n",
    "        iterations = 0\n",
    "        if initial_B is not None:\n",
    "            B = initial_B\n",
    "        else:\n",
    "            B = np.zeros(len(X[0]))\n",
    "        gradient = np.zeros(len(X[0]))\n",
    "        SS_past_gradients = np.zeros(len(X[0]))\n",
    "        while iterations < max_iterations and np.isinf(B).sum() == 0 and \\\n",
    "              (iterations == 0 or (eta * (gradient ** 2)).sum() > tol):\n",
    "            # calls the regression function\n",
    "            gradient = reg_func(X, Y, B, *reg_func_args)\n",
    "            \n",
    "            # Where SS_past_gradients is sum of squares of past gradients\n",
    "            SS_past_gradients += gradient ** 2\n",
    "            B = B - ((eta * gradient) / (np.sqrt(SS_past_gradients) + 1e-8))\n",
    "            \n",
    "            iterations += 1\n",
    "\n",
    "        if iterations < max_iterations and np.isinf(B).sum() == 0 and np.isnan(B).sum() == 0:\n",
    "            print(f'Gradient converged w/ {iterations} iterations and eta = {eta}')\n",
    "            np.seterr(all=\"warn\")\n",
    "            return B\n",
    "        print(f'Eta: {eta}; Iterations: {iterations}')\n",
    "    print('GRADIENT DID NOT CONVERGE. RESULTS ARE BAD')\n",
    "    np.seterr(all=\"warn\")\n",
    "    return B"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
