{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-21T18:17:48.135418Z",
     "start_time": "2022-10-21T18:17:48.133240Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#pd.read_csv('application_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Implementation (Each function computes the current gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-22T06:16:35.438231Z",
     "start_time": "2022-10-22T06:16:35.432770Z"
    }
   },
   "outputs": [],
   "source": [
    "# Logistic Regression Gradient\n",
    "def logistic(X, Y, B):\n",
    "    half =  np.exp(X @ B)\n",
    "    p = half / (1 + half)\n",
    "    gradient = -1 * (Y - p).T @ X\n",
    "    \n",
    "    return gradient\n",
    "\n",
    "# Logistic Regression with Lasso Penalty\n",
    "def logistic_lasso(X, Y, B, lamb):\n",
    "    half =  np.exp(X @ B)\n",
    "    p = half / (1 + half)\n",
    "    partial_gradient = -1 * (Y - p).T @ X\n",
    "    \n",
    "    # Add on +/- lambda * B to the gradient\n",
    "    lamb_beta = np.nan_to_num(lamb * -1 * (B / (B * -1)))\n",
    "    gradient = partial_gradient + lamb_beta\n",
    "    \n",
    "    return gradient\n",
    "\n",
    "def predict_prob(X, B):\n",
    "    return 1 / (1 + np.exp(-1 * X @ B))\n",
    "\n",
    "\n",
    "def SVC(X, Y, B, lamb):\n",
    "    # REQUIRES CLASS LABELS 1 / -1\n",
    "    Y = (Y - 1) + Y\n",
    "    \n",
    "    # I know this looks crazy but trust it works :)\n",
    "    mask = (1 - (Y * (X @ B))) <= 0\n",
    "\n",
    "    if_not_0_replace_w = B - (lamb * (X.T * Y).T)\n",
    "    return (B * mask.sum() + if_not_0_replace_w[~mask].sum(axis=0)) / len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-21T20:48:30.395983Z",
     "start_time": "2022-10-21T20:48:30.388473Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.5, 7.5])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = np.array([1, -1, 1, -1])\n",
    "X = np.array([[1, 2], [1, 3], [1, 4], [1, 15]])\n",
    "B = np.array([5, 3])\n",
    "\n",
    "if_not_0_replace_w = B - (X.T * Y).T\n",
    "\n",
    "mask = (1 - (Y * (X @ B))) <= 0\n",
    "B * mask.sum()\n",
    "(B * mask.sum() + if_not_0_replace_w[~mask].sum(axis=0)) / len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-21T18:16:40.032110Z",
     "start_time": "2022-10-21T18:16:38.774298Z"
    }
   },
   "outputs": [],
   "source": [
    "heart = pd.read_csv('https://www.dropbox.com/s/jpnyx41u7wpa41m/heart_attack_clean.csv?dl=1')\n",
    "heart_X = heart.drop(columns=['output'])\n",
    "heart_X['B0'] = 1\n",
    "heart_X = heart_X[['B0'] + list(heart_X.drop(columns='B0').columns)]\n",
    "heart_X = heart_X.to_numpy()\n",
    "heart_Y = heart['output'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-22T06:18:11.858686Z",
     "start_time": "2022-10-22T06:18:04.106704Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eta: 0.1; Iterations: 75000\n",
      "Eta: 0.01; Iterations: 75000\n",
      "Gradient converged w/ 1682 iterations and eta = 0.001\n",
      "Accuracy: 0.791; Precision: 0.799; Recall: 0.815; f1: 0.807\n"
     ]
    }
   ],
   "source": [
    "heart_SVC_B = gradient_descent(SVC, heart_X, heart_Y, 10)\n",
    "p_logistic = predict_prob(heart_X, heart_SVC_B)\n",
    "compute_metrics(p_logistic, heart_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-22T06:16:32.978587Z",
     "start_time": "2022-10-22T06:16:32.970976Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8021978021978022"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "clf = sklearn.svm.SVC(kernel='linear')\n",
    "clf.fit(heart_X, heart_Y)\n",
    "clf.score(heart_X, heart_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-21T18:16:40.708836Z",
     "start_time": "2022-10-21T18:16:40.701633Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_accuracy(p, Y):\n",
    "    return sum((p > .5) == Y) / len(Y)\n",
    "\n",
    "def get_precision(p, Y):\n",
    "    # precision = TP / (TP + FP)\n",
    "    TP = np.where(Y, ((p > .5) == Y), False).sum()\n",
    "    FP = np.where(Y == 0, (p > .5), False).sum()\n",
    "    return TP / (TP + FP)\n",
    "   \n",
    "def get_recall(p, Y):\n",
    "    # recall = TP / (TP + FN)\n",
    "    TP = np.where(Y, ((p > .5) == Y), False).sum()\n",
    "    FN = np.where(Y, ((p > .5) != Y), False).sum()\n",
    "    return TP / (TP + FN)\n",
    "\n",
    "def get_f1(p, Y):\n",
    "    precision = get_precision(p, Y)\n",
    "    recall = get_recall(p, Y)\n",
    "    return 2 / ((1 / precision) + (1 / recall))\n",
    "    \n",
    "# Compute Accuracy, precision, recall, and f1\n",
    "def compute_metrics(p, Y):\n",
    "    # precision = TP / (TP + FP)\n",
    "    # recall = TP / (TP + FN)\n",
    "    accuracy = sum((p > .5) == Y) / len(Y)\n",
    "    \n",
    "    TP = np.where(Y, ((p > .5) == Y), False).sum()\n",
    "    FP = np.where(Y == 0, (p > .5), False).sum()\n",
    "    precision = TP / (TP + FP)\n",
    "    \n",
    "    FN = np.where(Y, ((p > .5) != Y), False).sum()\n",
    "    recall = TP / (TP + FN)\n",
    "    \n",
    "    f1 = 2 / ((1 / precision) + (1 / recall))\n",
    "    print(f\"Accuracy: {round(accuracy, 3)}; Precision: {round(precision, 3)}; \" + \\\n",
    "           f\"Recall: {round(recall, 3)}; f1: {round(f1, 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Implementation (Pass in a regression function, X, Y, and any optional arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-21T18:17:32.220126Z",
     "start_time": "2022-10-21T18:17:32.210148Z"
    }
   },
   "outputs": [],
   "source": [
    "def gradient_descent(reg_func, X, Y, *reg_func_args, initial_B=None, max_iterations=75000, tol = .00001,\n",
    "                     etas=[.1, .01, .001, .0001, .00001, .000001], err=False):\n",
    "    if not err:\n",
    "        np.seterr(all=\"ignore\")\n",
    "    else:\n",
    "        np.seterr(all=\"warn\")\n",
    "    \n",
    "    for eta in etas:\n",
    "        # reset\n",
    "        iterations = 0\n",
    "        if initial_B is not None:\n",
    "            B = initial_B\n",
    "        else:\n",
    "            B = np.zeros(len(X[0]))\n",
    "        gradient = np.zeros(len(X[0]))\n",
    "        while iterations < max_iterations and np.isinf(B).sum() == 0 and \\\n",
    "              (iterations == 0 or (eta * (gradient ** 2)).sum() > tol):\n",
    "            # calls the regression function\n",
    "            gradient = reg_func(X, Y, B, *reg_func_args)\n",
    "            B = B - (eta * gradient)\n",
    "            iterations += 1\n",
    "\n",
    "        if iterations < max_iterations and np.isinf(B).sum() == 0 and np.isnan(B).sum() == 0:\n",
    "            print(f'Gradient converged w/ {iterations} iterations and eta = {eta}')\n",
    "            np.seterr(all=\"warn\")\n",
    "            return B\n",
    "        print(f'Eta: {eta}; Iterations: {iterations}')\n",
    "    print('GRADIENT DID NOT CONVERGE. RESULTS ARE BAD')\n",
    "    np.seterr(all=\"warn\")\n",
    "    return B\n",
    "\n",
    "# The code below uses the 'Adagrad' gradient descent optimization algorithm to adapt the \n",
    "# learning rate for each dimension. There are other versions of this that may be more effective\n",
    "# Directions as to how this works as well as other ideas: \n",
    "# https://ruder.io/optimizing-gradient-descent/index.html#momentum\n",
    "def adaptive_gradient_descent(reg_func, X, Y, *reg_func_args, initial_B=None, max_iterations=100000, \n",
    "                              tol = .001, etas=[.1], err=False):\n",
    "    if not err:\n",
    "        np.seterr(all=\"ignore\")\n",
    "    else:\n",
    "        np.seterr(all=\"warn\")\n",
    "    \n",
    "    for eta in etas:\n",
    "        # reset\n",
    "        iterations = 0\n",
    "        if initial_B is not None:\n",
    "            B = initial_B\n",
    "        else:\n",
    "            B = np.zeros(len(X[0]))\n",
    "        gradient = np.zeros(len(X[0]))\n",
    "        SS_past_gradients = np.zeros(len(X[0]))\n",
    "        while iterations < max_iterations and np.isinf(B).sum() == 0 and \\\n",
    "              (iterations == 0 or (eta * (gradient ** 2)).sum() > tol):\n",
    "            # calls the regression function\n",
    "            gradient = reg_func(X, Y, B, *reg_func_args)\n",
    "            \n",
    "            # Where SS_past_gradients is sum of squares of past gradients\n",
    "            SS_past_gradients += gradient ** 2\n",
    "            #print(B)\n",
    "            B = B - ((eta * gradient) / (np.sqrt(SS_past_gradients) + 1e-8))\n",
    "            \n",
    "            iterations += 1\n",
    "\n",
    "        if iterations < max_iterations and np.isinf(B).sum() == 0 and np.isnan(B).sum() == 0:\n",
    "            print(f'Gradient converged w/ {iterations} iterations and eta = {eta}')\n",
    "            np.seterr(all=\"warn\")\n",
    "            return B\n",
    "        print(f'Eta: {eta}; Iterations: {iterations}')\n",
    "    print('GRADIENT DID NOT CONVERGE. RESULTS ARE BAD')\n",
    "    np.seterr(all=\"warn\")\n",
    "    return B"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
